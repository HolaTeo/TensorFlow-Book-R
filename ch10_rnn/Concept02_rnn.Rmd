---
title: 'Ch 10: Concept 02'
output: github_document
---

# Recurrent Neural Network

Import the relevant libraries:

```{r}
library(tensorflow)
```

Define the RNN model:

```{r}
SeriesPredictor <- setRefClass("SeriesPredictor",
    fields=c('input_dim', 'seq_size', 'hidden_dim', 'W_out', 'b_out',
             'x','y', 'cost', 'train_op', 'saver'),
    methods=list(
      initialize=function(input_dim, seq_size, hidden_dim=10){
        # Hyperparameters
        .self$input_dim <- as.integer(input_dim)
        .self$seq_size <- as.integer(seq_size)
        .self$hidden_dim <- as.integer(hidden_dim)
        
        # Weight variables and input placeholders
        .self$W_out <- tf$Variable(tf$random_normal(list(.self$hidden_dim, 1L)), name='W_out')
        .self$b_out <- tf$Variable(tf$random_normal(list(1L)), name='b_out')
        .self$x <- tf$placeholder(dtype=tf$float32, shape=list(NULL, .self$seq_size, .self$input_dim ))
        .self$y <- tf$placeholder(dtype=tf$float32, shape=list(NULL, .self$seq_size))
        
        # Cost optimizer
        .self$cost <- tf$reduce_mean(tf$square(model() - .self$y))
        .self$train_op <- tf$train$AdamOptimizer()$minimize(.self$cost)

        # Auxiliary ops
        .self$saver = tf$train$Saver()
      },
      model=function(){
        #:param x: inputs of size [T, batch_size, input_size]
        #:param W: matrix of fully-connected output layer weights
        #:param b: vector of fully-connected output layer biases
        cell <- tf$contrib$rnn$BasicLSTMCell(.self$hidden_dim)
        outputs_states <- tf$nn$dynamic_rnn(cell, .self$x, dtype=tf$float32)
        
        #not num_examples <- tf$shape(.self$x)[1]
        num_examples <- tf$shape(.self$x)[0]
        #expend : 10,1 -> 1,10,1 
        #tile : 1,10,1 -> 3,10,1
        W_repeated <- tf$tile(tf$expand_dims(.self$W_out, 0L), list(num_examples, 1L, 1L)) 
        #3,4,10 x 3,10,1
        out <- tf$matmul(outputs_states[[1]], W_repeated) + .self$b_out
        out <- tf$squeeze(out)
        return(out)
      },
      train=function(train_x, train_y){
         with(tf$Session() %as% sess, {
           tf$get_variable_scope()$reuse_variables()
           sess$run(tf$global_variables_initializer())
           for(i in 1:1000){
             mse_ <- sess$run(list(.self$train_op, .self$cost), feed_dict=dict(x= train_x, y= train_y))
             if(i %% 100){
               print(paste(i, mse_[[2]]))
             }
             save_path <- .self$saver$save(sess, 'model.ckpt')
             print(sprintf('Model saved to %s',save_path))
           }
         })
      },
      test=function(test_x){
         with(tf$Session() %as% sess, {
           tf$get_variable_scope()$reuse_variables()
           .self$saver$restore(sess, './model.ckpt')
           output <- sess$run(.self$model(), feed_dict=dict(x= test_x))
         })
        return(output)
      }
      )
    )
```

Now, we'll train a series predictor. Let's say we have a sequence of numbers [a, b, c, d] that we want to transform into [a, a+b, b+c, c+d]. We'll give the RNN a couple examples in the training data. Let's see how well it learns this intended transformation:



```{r}
predictor <- SeriesPredictor$new(input_dim=1, seq_size=4, hidden_dim=10)


train_x <- aperm(array(c(1,2,5,6,
                   5,7,7,8,
                   3,4,5,7),dim=c(4,3,1)),c(2,1,3))


train_y <- matrix(c(1,3, 7, 11,
                    5,12,14,15,
                    3,7, 9 ,12), nrow=3, byrow=T)

predictor$train(train_x, train_y)

test_x <- aperm(array(c(1,2,3,4,                         # 1,3,5,7
                        4,5,6,7),dim=c(4,2,1)), c(2,1,3))# 4,9,11,13

actual_y<- matrix(c(1,3,5,7,
                    4,9,11,13), nrow=2, byrow=T)

pred_y <- predictor$test(test_x)

cat("\nLets run some tests!\n")

for(i in 1:length(test_x)){
        print(sprintf("When the input is %f",as.vector(test_x)[i]))
        print(sprintf("The ground truth output should be %f", as.vector(actual_y)[i]))
        print(sprintf("And the model thinks it is %f", as.vector(pred_y)[i]))
        cat('\n')
}
```

