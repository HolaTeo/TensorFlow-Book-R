---
title: "PCA to Autoencoder vs SIR"
author: "Choi Taeyoung"
date: \today
output: pdf_document
---

# Autoencoder

All we'll need is TensorFlow and NumPy:
```{r}
library(tensorflow)
library(numpy)
```

Instead of feeding all the training data to the training op, we will feed data in small batches:

```{r}
get_batch <- function(X, size){
  a <- sample(1:nrow(X), size)
  return(X[a,])
}

```


Define the autoencoder class:

```{r}
AE_PCA <- setRefClass("Autoencoder",
  fields = c('input_dim', 'hidden_dim', 'epoch', 'batch_size', 
             'learning_rate', 'x', 'encoded', 'decoded', 'loss', 'all_loss',
             'train_op','saver', 'autoencoder_graph'),
  methods=list(
    initialize=function(input_dim, hidden_dim, epoch=500, batch_size=10, learning_rate=0.001){
      .self$epoch <- as.integer(epoch)
      .self$batch_size <- as.integer(batch_size)
      .self$learning_rate <- learning_rate
      .self$input_dim <- as.integer(input_dim)
      .self$hidden_dim <- as.integer(hidden_dim)
      
      # make graph for avoiding restore error 
      .self$autoencoder_graph <- tf$Graph()
      
      with(.self$autoencoder_graph$as_default(), {
        .self$x <- tf$placeholder(dtype=tf$float32, shape=list(NULL, .self$input_dim))
        with(tf$name_scope('encode'), {
              encode_weights <- tf$Variable(tf$random_normal(list(.self$input_dim, .self$hidden_dim),
                                                      dtype=tf$float32), name='weights')
              encode_biases <- tf$Variable(tf$zeros(list(.self$hidden_dim)), name='biases')
              .self$encoded  <- tf$nn$sigmoid(tf$matmul(x, encode_weights) + encode_biases)
        })

        # Define a saver op
        .self$saver = tf$train$Saver()
      })
      },
    train=function(data){
      with(tf$Session(graph=.self$autoencoder_graph) %as% sess,{
        sess$run(tf$global_variables_initializer())
        for(i in 1:.self$epoch){
          for(j in 1:500){
            batch_data <- get_batch(data, .self$batch_size)
            l_ <- sess$run(list(.self$loss, .self$train_op), feed_dict=dict(x=batch_data))
          }
          if(i %% 50 == 0){
            print(sprintf('epoch %d: loss = %f', i, l_[[1]]))
            .self$saver$save(sess, './pca.ckpt')
          }
        }
        .self$saver$save(sess, './pca.ckpt')
        })
      },
    test=function(data){
      with(tf$Session(graph=.self$autoencoder_graph) %as% sess, {
        .self$saver$restore(sess, './pca.ckpt')
        hidden_reconstructed <- sess$run(list(.self$encoded), feed_dict=dict(x=data))
        })
      print(paste('input', data))
      print(paste('compressed', hidden_reconstructed[[1]]))
      return(hidden_reconstructed[[1]])
      },
    get_params=function(){
      with(tf$Session(graph=.self$autoencoder_graph) %as% sess, {
        .self$saver$restore(sess, './pca.ckpt')
        weight_biases <- sess$run(list(.self$weight1, .self$biases1))
      })
      return(list(weight=weight_biases[[1]], biases=weight_biases[[2]]))
      },
    classify=function(data, labels){
      with(tf$Session(graph=.self$autoencoder_graph) %as% sess, {
        sess$run(tf$global_variables_initializer())
        .self$saver$restore(sess, './pca.ckpt')
        hidden_reconstructed <- sess$run(list(.self$encoded), feed_dict=dict(x=data))
        reconstructed <- hidden_reconstructed[[1]]
        print(dim(reconstructed))
      })
      return(hidden_reconstructed[[1]])
      }
    ))

# Analysis
pca_indices <- which(labels == 7)
pca_x = x[pca_indices,]
print(dim(pca_x)) 

input_dim <- 10
hidden_dim <- 2


aep <- AE_PCA$new(input_dim, hidden_dim)
aep$train()
```

```{r}
# import tensorflow as tf
set.seed(4)
m <-  200
w1 <- 0.1
w2 <- 0.3
w12 <- c(w1, w2)
noise <-  0.1
#get angle
angles <- NULL
for(i in 1:m){
  a1 <-  runif(i) * 3 * pi / 2 - 0.5
  angles <- cbind(a1) %>% as.vector()
}

# get sample data
data <- c(NULL, NULL, NULL)
for (a in 1 : m){
  m1 <- cos(angles[a]) + sin(angles[a])/2 + noise * rnorm(a) / 2
  m2 <- sin(angles[a]) * 0.7 + noise * rnorm(a) / 2
  m3 <- m1 * w1 + m2 * w2 + noise * rnorm(a)
  data <- cbind(m1,m2,m3)
}

X_train <- data[1:100,]
X_test <- data[101:200,]


################ # layer params # ################ 
n_inputs = 3 
n_hidden = 2 # coding units 
n_outputs = n_inputs 

# autoencoder 
X <-  tf$placeholder(tf$float32, shape=list(NULL, n_inputs))
hidden <-  tf$layers$dense(X, n_hidden) 
outputs <-  tf$layers$dense(hidden, n_outputs) 

################ # Train params # ################
learning_rate <-  0.01 
n_iterations <-  1000 
pca <-  hidden 

# loss
reconstruction_loss <-  tf$reduce_mean(tf$square(outputs - X)) # MSE
# optimizer 
train_op <- tf$train$AdamOptimizer(learning_rate)$minimize(reconstruction_loss)

  with(tf$Session()) %as% sess ,{
    sess$run(tf$global_variables_initializer())
    for(i in 1:n_iterations){
      train_op$run(feed_dict={X=X_train})
      pca_val <-  pca.eval(feed_dict={X=X_test})
      }
    }
  }


with tf.Session() as sess: 
  tf.global_variables_initializer().run() 
  for iteration in range(n_iterations):
    train_op.run(feed_dict={X: X_train}) 
  pca_val = pca.eval(feed_dict={X: X_test})

test=function(data){
      with(tf$Session(graph=.self$autoencoder_graph) %as% sess, {
        .self$saver$restore(sess, './pca.ckpt')
        hidden_reconstructed <- sess$run(list(.self$encoded), feed_dict=dict(x=data))
        })
      print(paste('input', data))
      print(paste('compressed', hidden_reconstructed[[1]]))
      return(hidden_reconstructed[[1]])
      }
```

